{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BigTMiami/colabtest/blob/main/Colab_Machine_Translation_LSTM_TUNING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "dfJtyp5Kefgy"
      },
      "source": [
        "\n",
        "*Machine Translation Jupyter Notebook.  (c) 2021 Georgia Tech*\n",
        "\n",
        "*Copyright 2021, Georgia Institute of Technology (Georgia Tech) <br>Atlanta, Georgia 30332<br>All Rights Reserved*\n",
        "\n",
        "*Georgia Tech asserts copyright ownership of this template and all derivative works, including solutions to the projects assigned in this course. Students and other users of this template code are advised not to share it with others or to make it available on publicly viewable websites including repositories such as Github, Bitbucket, and Gitlab.  This copyright statement should not be removed or edited.*\n",
        "\n",
        "*Sharing solutions with current or future students of CS 7643 Deep Learning is prohibited and subject to being investigated as a GT honor code violation.*\n",
        "\n",
        "*DO NOT EDIT ANYTHING ABOVE THIS LINE*\n",
        "\n",
        "# Machine Translation with Seq2Seq and Transformers\n",
        "In this exercise you will implement a [Sequence to Sequence(Seq2Seq)](https://arxiv.org/abs/1703.03906) and a [Transformer](https://arxiv.org/pdf/1706.03762.pdf) model and use them to perform machine translation.\n",
        "\n",
        "**A quick note: if you receive the following TypeError \"super(type, obj): obj must be an instance or subtype of type\", try re-importing that part or restarting your kernel and re-running all cells.** Once you have finished making changes to the model constuctor, you can avoid this issue by commenting out all of the model instantiations after the first (e.g. lines starting with \"model = TransformerTranslator(*args, **kwargs)\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmSlhfxwefg9"
      },
      "source": [
        "# ** 1: Introduction**\n",
        "\n",
        "## Multi30K: Multilingual English-German Image Descriptions\n",
        "\n",
        "[Multi30K](https://github.com/multi30k/dataset) is a dataset for machine translation tasks. It is a multilingual corpus containing English sentences and their German translation. In total it contains 31014 sentences(29000 for training, 1014 for validation, and 1000 for testing).\n",
        "As one example:\n",
        "\n",
        "En: `Two young, White males are outside near many bushes.`\n",
        "\n",
        "De: `Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.`\n",
        "\n",
        "You can read more info about the dataset [here](https://arxiv.org/abs/1605.00459). The following parts of this assignment will be based on this dataset.\n",
        "\n",
        "## TorchText: A PyTorch Toolkit for Text Dataset and NLP Tasks\n",
        "[TorchText](https://github.com/pytorch/text) is a PyTorch package that consists of data processing utilities and popular datasets for natural language. They serve to help with data splitting and loading, token encoding, sequence padding, etc. You don't need to know about how TorchText works in detail, but you might want to know about why those classes are needed and what operations are necessary for machine translation. This knowledge can be migrated to all sequential data modeling. In the following parts, we will provide you with some code to help you understand.\n",
        "\n",
        " You can refer to torchtext's documentation(v0.9.0) [here](https://pytorch.org/text/).\n",
        "\n",
        "## Spacy\n",
        "Spacy is package designed for tokenization in many languages. Tokenization is a process of splitting raw text data into lists of tokens that can be further processed. Since TorchText only provides tokenizer for English, we will be using Spacy for our assignment.\n",
        "\n",
        "\n",
        "**Notice: For the following assignment, we strongly recommend you to work in a virtual python environment. We recommend Anaconda, a powerful environment control tool. You can download it [here](https://www.anaconda.com/products/individual)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O66UKOZnefg_"
      },
      "source": [
        "## ** 1.1: Prerequisites**\n",
        "Before you start this assignment, you need to have all required packages installed either on the terminal you are using, or in the virtual environment. Please make sure you have the following package installed:\n",
        "\n",
        "`PyTorch, TorchText, Spacy, Tqdm, Numpy`\n",
        "\n",
        "You can first check using either `pip freeze` in terminal or `conda list` in conda environment. Then run the following code block to make sure they can be imported."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "909NjxVQShyF"
      },
      "outputs": [],
      "source": [
        "%pip install torchtext torch  spacy tqdm numpy jupyter notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLFFO34STMy4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "%cd '/content/drive/MyDrive/assignment4_spring24'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb7cFwJvNSdU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# add for trace error improvement\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9wH5bYVefhA"
      },
      "outputs": [],
      "source": [
        "# Just run this block. Please do not modify the following code.\n",
        "import math\n",
        "import time\n",
        "import io\n",
        "import numpy as np\n",
        "import csv\n",
        "from IPython.display import Image\n",
        "\n",
        "# Pytorch package\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Torchtest package\n",
        "import torchtext\n",
        "from torchtext.datasets import Multi30k\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import vocab\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Tqdm progress bar\n",
        "from tqdm import tqdm_notebook, tqdm\n",
        "\n",
        "# Code provide to you for training and evaluation\n",
        "from utils import train, evaluate, set_seed_nb, unit_test_values, deterministic_init, plot_curves\n",
        "\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5malPk_1efhD"
      },
      "source": [
        "Once you properly import the above packages, you can proceed to download Spacy English and German tokenizers by running the following command in your **terminal**. They will take some time.\n",
        "\n",
        "`python -m spacy download en_core_web_sm`\n",
        "\n",
        "`python -m spacy download de_core_news_sm`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_8L7gLDMBK2"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CslQ00wNefhE"
      },
      "source": [
        "Now lets check your GPU availability and load some sanity checkers. By default you should be using your gpu for this assignment if you have one available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHOKA0U5efhF"
      },
      "outputs": [],
      "source": [
        "# Check device availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"You are using device: %s\" % device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJXBPulgefhH"
      },
      "outputs": [],
      "source": [
        "# load checkers\n",
        "d1 = torch.load('./data/d1.pt')\n",
        "d2 = torch.load('./data/d2.pt')\n",
        "d3 = torch.load('./data/d3.pt')\n",
        "d4 = torch.load('./data/d4.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDmu06h_efhI"
      },
      "source": [
        "## **1.2: Preprocess Data**\n",
        "With TorchText and Spacy tokenizers ready, you can now prepare the data using *TorchText* objects. Just run the following code blocks. Read the comment and try to understand what they are for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWW68l46efhJ"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 20\n",
        "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
        "train_urls = ('train.de.gz', 'train.en.gz')\n",
        "val_urls = ('val.de.gz', 'val.en.gz')\n",
        "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
        "\n",
        "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
        "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
        "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n",
        "\n",
        "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5Rx2dPtefhK"
      },
      "outputs": [],
      "source": [
        "def build_vocab(filepath, tokenizer):\n",
        "  counter = Counter()\n",
        "  with io.open(filepath, encoding=\"utf8\") as f:\n",
        "    for string_ in f:\n",
        "      counter.update(tokenizer(string_.lower()))\n",
        "  return vocab(counter, specials=['<unk>', '<pad>', '<sos>', '<eos>'], min_freq=2)\n",
        "\n",
        "\n",
        "de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
        "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)\n",
        "de_vocab.set_default_index(de_vocab['<unk>'])\n",
        "en_vocab.set_default_index(en_vocab['<unk>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCIRekFBefhL"
      },
      "outputs": [],
      "source": [
        "def data_process(filepaths):\n",
        "  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "  data = []\n",
        "  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
        "    raw_en_l=raw_en.lower()     #turn sentences to lower case\n",
        "    raw_de_l=raw_de.lower()\n",
        "    de_tensor = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de_l)],\n",
        "                            dtype=torch.long)\n",
        "    en_tensor = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en_l)],\n",
        "                            dtype=torch.long)\n",
        "    if len(de_tensor) <= MAX_LEN-2 and len(en_tensor) <= MAX_LEN-2:\n",
        "        data.append((de_tensor, en_tensor))\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SH0hV4WzefhM"
      },
      "outputs": [],
      "source": [
        "train_data = data_process(train_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "test_data = data_process(test_filepaths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCMNnBQDefhM"
      },
      "outputs": [],
      "source": [
        "PAD_IDX = de_vocab['<pad>']\n",
        "SOS_IDX = de_vocab['<sos>']\n",
        "EOS_IDX = de_vocab['<eos>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ew_LjPRuefhO"
      },
      "outputs": [],
      "source": [
        "def generate_batch(data_batch):\n",
        "\n",
        "    de_batch, en_batch = [], []\n",
        "    for (de_item, en_item) in data_batch:\n",
        "          en_batch.append(torch.cat([torch.tensor([SOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "          de_batch.append(torch.cat([torch.tensor([SOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
        "    de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
        "    fix=torch.ones(MAX_LEN,en_batch.shape[1])\n",
        "    two= pad_sequence([de_batch,en_batch, fix], padding_value=PAD_IDX)\n",
        "    de_batch=two[:,0,]\n",
        "    en_batch=two[:,1,]\n",
        "    return de_batch, en_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSE8cUDJefhP"
      },
      "outputs": [],
      "source": [
        "# Get the input and the output sizes for model\n",
        "input_size = len(de_vocab)\n",
        "output_size = len(en_vocab)\n",
        "print (input_size,output_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7zKygJLefhW"
      },
      "source": [
        "# **3: Train a Seq2Seq Model**\n",
        "In this section, you will be working on implementing a simple Seq2Seq model. You will first implement an Encoder and a Decoder, and then join them together with a Seq2Seq architecture. You will need to complete the code in *Decoder.py*, *Encoder.py*, and *Seq2Seq.py* under *seq2seq* folder. Please refer to the instructions in those files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBXsJFeTefhZ"
      },
      "source": [
        "## **3.3: Implement the Seq2Seq**\n",
        "In this section you will be implementing the Seq2Seq model that utilizes the Encoder and Decoder you implemented. Please refer to the instructions in *seq2seq/Seq2Seq.py*. Run the following block to check your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPijQP0eJFCi"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "def save_experiment_results(filename, results_dir=\"results/\", **kwargs):\n",
        "    kwargs[\"save_time\" ] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    with open(results_dir + filename + \".json\", 'w') as f:\n",
        "        json.dump(kwargs, f, indent=4)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzf0Jr66JOYe"
      },
      "outputs": [],
      "source": [
        "def train_and_plot (model, optimizer, scheduler, criterion, filename, EPOCHS, train_loader,valid_loader):\n",
        "  train_perplexity_history = []\n",
        "  valid_perplexity_history = []\n",
        "\n",
        "  for epoch_idx in range(EPOCHS):\n",
        "      print(\"-----------------------------------\")\n",
        "      print(\"Epoch %d\" % (epoch_idx+1))\n",
        "      print(\"-----------------------------------\")\n",
        "\n",
        "      train_loss, avg_train_loss = train(model, train_loader, optimizer, criterion, device=device)\n",
        "      scheduler.step(train_loss)\n",
        "\n",
        "      val_loss, avg_val_loss = evaluate(model, valid_loader, criterion, device=device)\n",
        "\n",
        "      train_perplexity_history.append(np.exp(avg_train_loss))\n",
        "      valid_perplexity_history.append(np.exp(avg_val_loss))\n",
        "\n",
        "      print(\"Training Loss: %.4f. Validation Loss: %.4f. \" % (avg_train_loss, avg_val_loss))\n",
        "      print(\"Training Perplexity: %.4f. Validation Perplexity: %.4f. \" % (np.exp(avg_train_loss), np.exp(avg_val_loss)))\n",
        "\n",
        "  plot_curves(train_perplexity_history, valid_perplexity_history, filename)\n",
        "\n",
        "  return avg_train_loss, avg_val_loss, np.exp(avg_train_loss), np.exp(avg_val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmS8Jk_rJdTf"
      },
      "outputs": [],
      "source": [
        "def create_token_string(word_tensor, example_count=3, vocab=en_vocab):\n",
        "    token_strings = []\n",
        "    for i in range(example_count):\n",
        "        target_words = vocab.lookup_tokens(word_tensor[i].tolist())\n",
        "        translation_string = \"\"\n",
        "        for word in target_words:\n",
        "            if word in ['<sos>', '<eos>', '<pad>', '\\n']:\n",
        "                continue\n",
        "            translation_string += word + \" \"\n",
        "        token_strings.append(translation_string)\n",
        "    return  token_strings\n",
        "\n",
        "\n",
        "\n",
        "def create_translation_string(model, dataloader, example_count=3 ):\n",
        "    with torch.no_grad():\n",
        "        data = next(iter(dataloader))\n",
        "        source = data[0].transpose(1, 0).to(device)\n",
        "        target = data[1].transpose(1, 0).to(device)\n",
        "        batch_size, seq_len = target.shape\n",
        "        translation = model(source)\n",
        "        translation = translation.reshape(batch_size * seq_len, translation.shape[-1])\n",
        "        translation = torch.argmax(translation, dim=1)\n",
        "        translation = translation.reshape(batch_size,seq_len)\n",
        "\n",
        "    target_examples = create_token_string(target, example_count)\n",
        "    translation_examples = create_token_string(translation, example_count)\n",
        "\n",
        "    return target_examples, translation_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-BUK199yWXK"
      },
      "outputs": [],
      "source": [
        "%autoreload 2\n",
        "from models.seq2seq.Encoder import Encoder\n",
        "from models.seq2seq.Decoder import Decoder\n",
        "from models.seq2seq.Seq2Seq import Seq2Seq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffuhd68VXp_u"
      },
      "outputs": [],
      "source": [
        "def run_training(**kwargs):\n",
        "  encoder_emb_size = kwargs[\"encoder_emb_size\"] if \"encoder_emb_size\" in kwargs else 128\n",
        "  encoder_hidden_size = kwargs[\"encoder_hidden_size\"] if \"encoder_hidden_size\" in kwargs else 128\n",
        "  encoder_dropout = kwargs[\"encoder_dropout\"] if \"encoder_dropout\" in kwargs else 0.2\n",
        "\n",
        "  decoder_emb_size = kwargs[\"decoder_emb_size\"] if \"decoder_emb_size\" in kwargs else 128\n",
        "  decoder_hidden_size = kwargs[\"decoder_hidden_size\"] if \"decoder_hidden_size\" in kwargs else 128\n",
        "  decoder_dropout = kwargs[\"decoder_dropout\"] if \"decoder_dropout\" in kwargs else 0.2\n",
        "\n",
        "  learning_rate = kwargs[\"learning_rate\"] if \"learning_rate\" in kwargs else 0.001\n",
        "  model_type = kwargs[\"model_type\"] if \"model_type\" in kwargs else \"LSTM\"\n",
        "  EPOCHS = kwargs[\"EPOCHS\"] if \"EPOCHS\" in kwargs else 20\n",
        "\n",
        "  attention = kwargs[\"attention\"] if \"attention\" in kwargs else True\n",
        "  BATCH_SIZE = kwargs[\"BATCH_SIZE\"] if \"BATCH_SIZE\" in kwargs else 128\n",
        "  EPOCHS = kwargs[\"EPOCHS\"] if \"EPOCHS\" in kwargs else 20\n",
        "\n",
        "  train_loader = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                          shuffle=False, collate_fn=generate_batch)\n",
        "  valid_loader = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
        "                          shuffle=False, collate_fn=generate_batch)\n",
        "  test_loader = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=False, collate_fn=generate_batch)\n",
        "\n",
        "  #input size and output size\n",
        "  input_size = len(de_vocab)\n",
        "  output_size = len(en_vocab)\n",
        "\n",
        "  # Declare models, optimizer, and loss function\n",
        "  set_seed_nb()\n",
        "  encoder = Encoder(input_size, encoder_emb_size, encoder_hidden_size, decoder_hidden_size, dropout = encoder_dropout, model_type = model_type)\n",
        "  decoder = Decoder(decoder_emb_size, encoder_hidden_size, encoder_hidden_size, output_size, dropout = decoder_dropout, model_type = model_type)\n",
        "  seq2seq_model = Seq2Seq(encoder, decoder, device, attention=attention)\n",
        "\n",
        "  optimizer = optim.Adam(seq2seq_model.parameters(), lr = learning_rate)\n",
        "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "  filename=f'lstm_tuned_batch'\n",
        "  for key,value in kwargs.items():\n",
        "    filename += f\"_{key}_{value}\".replace('.','_')\n",
        "\n",
        "  print(f\"EPOCHS:{EPOCHS} filename:{filename}\")\n",
        "\n",
        "  train_loss, val_loss, train_perplexity, val_perplexity = train_and_plot(seq2seq_model, optimizer, scheduler, criterion, filename, EPOCHS,train_loader,valid_loader)\n",
        "  target_examples, translation_examples = create_translation_string(seq2seq_model, test_loader)\n",
        "  save_experiment_results(filename, train_loss=train_loss, val_loss=val_loss, train_perplexity=train_perplexity, val_perplexity=val_perplexity,\n",
        "                          input_size=input_size, output_size=output_size,\n",
        "                          encoder_emb_size=encoder_emb_size, encoder_hidden_size=encoder_hidden_size, encoder_dropout=encoder_dropout,\n",
        "                          decoder_emb_size=decoder_emb_size, decoder_hidden_size=decoder_hidden_size, decoder_dropout=decoder_dropout,\n",
        "                          learning_rate=learning_rate, model_type=model_type, EPOCHS=EPOCHS, attention=attention,\n",
        "                          target_examples=target_examples, translation_examples=translation_examples, BATCH_SIZE=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ojRdoNdEabxo"
      },
      "outputs": [],
      "source": [
        "# run_training(encoder_dropout=0.1)\n",
        "# run_training(encoder_dropout=0.4)\n",
        "# run_training(decoder_dropout=0.1)\n",
        "# run_training(decoder_dropout=0.4)\n",
        "# run_training(encoder_emb_size=64)\n",
        "# run_training(encoder_emb_size=256)\n",
        "# run_training(decoder_emb_size=64)\n",
        "# run_training(decoder_emb_size=256)\n",
        "run_training(decoder_hidden_size=64)\n",
        "run_training(decoder_hidden_size=256)\n",
        "run_training(encoder_emb_size=256,decoder_emb_size=64,decoder_dropout=0.3)\n",
        "run_training(EPOCH=80,BATCH_SIZE=256,learning_rate=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7-fSJik-xbH"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "dfJtyp5Kefgy"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "0e75f62e3678e2cc45ba815b06d45149f3ef8e725365fb50a06024c1d0abc38d"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}